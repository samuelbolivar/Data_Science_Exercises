{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA Data Science (DAT19) - Lab 18\n",
    "\n",
    "### Scratching the surface of NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-banner\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"0907b4e8-9b72-4d9e-8369-6cd21f695960\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.11.1.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#0907b4e8-9b72-4d9e-8369-6cd21f695960\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.11.1.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.11.1.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# usual imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure,show,output_notebook\n",
    "\n",
    "output_notebook()\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Each is a different implemntation of a text transform tool: Bag of Words & Tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be demonstrating this process on the standard sklearn text introduction (with one or two diversions). We'll be working with labeled review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "with open('../data/amazon_cells_labelled.txt') as f:\n",
    "    for i,line in enumerate(f.readlines()):\n",
    "        row = line.split('\\n')[0].split('\\t')\n",
    "        if row[1] == '':\n",
    "            row[1] = np.nan\n",
    "        else:\n",
    "            row[1] = int(row[1])\n",
    "        rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15004 entries, 0 to 15003\n",
      "Data columns (total 2 columns):\n",
      "text         15004 non-null object\n",
      "sentiment    1000 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.DataFrame(rows,columns=['text','sentiment'])\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 1 to 2937\n",
      "Data columns (total 2 columns):\n",
      "text         1000 non-null object\n",
      "sentiment    1000 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 23.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data = all_data.dropna()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is sentiment on Amazon reviews! But in order to use it we need to create a feature space. Please review the documentation for these functions:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "You'll see that we have a ton of options available. I'll start out with the simplest default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x1642 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 4702 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = count_vect = CountVectorizer(stop_words='english')\n",
    "bag_o_words = count_vect.fit_transform(data['text'])\n",
    "bag_o_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a sparse matrix! You can convert to dense matrices with `.todense()` (named for clarity, clearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1642)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_o_words.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fify',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note we can see what the words are!\n",
    "count_vect.get_feature_names()\n",
    "count_vect.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(bag_o_words,data['sentiment'], random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82999999999999996"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrongs = []\n",
    "for p,t,i in zip(clf.predict(X_test),y_test.values,y_test.index):\n",
    "    if p==t:\n",
    "        pass\n",
    "    else:\n",
    "        wrongs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950                                  These are fabulous!\n",
       "2768                                       Lousy product.\n",
       "2614    The pairing of the two devices was so easy it ...\n",
       "643     I did not bother contacting the company for fe...\n",
       "2843    You also cannot take pictures with it in the c...\n",
       "1838                                Better than expected.\n",
       "244     If you plan to use this in a car forget about it.\n",
       "987     Battery life still not long enough in Motorola...\n",
       "1778    My experience was terrible..... This was my fo...\n",
       "2570    It seems completely secure, both holding on to...\n",
       "1814    It does everything the description said it would.\n",
       "54      I have yet to run this new battery below two b...\n",
       "67      This is a simple little phone to use, but the ...\n",
       "2292          Plan on ordering from them again and again.\n",
       "2123    Couldn't use the unit with sunglasses, not goo...\n",
       "1012    As many people complained, I found this headse...\n",
       "2016    The volume for the ringer is REAL good (you ha...\n",
       "2072                                        Not worth it.\n",
       "2316                          Not as good as I had hoped.\n",
       "827      The cable looks so thin and flimsy, it is scary.\n",
       "2303    They keep getting better and better (this is m...\n",
       "2837    There was so much hype over this phone that I ...\n",
       "1355                                         Low Quality.\n",
       "2775    It fits so securely that the ear hook does not...\n",
       "1727                 None of it works, just don't buy it.\n",
       "2082    My phone doesn't slide around my car now and t...\n",
       "2644    My phone sounded OK ( not great - OK), but my ...\n",
       "779        It defeats the purpose of a bluetooth headset.\n",
       "2774               :-)Oh, the charger seems to work fine.\n",
       "1350    My Sanyo has survived dozens of drops on black...\n",
       "2168    I really wanted the Plantronics 510 to be the ...\n",
       "2874    The text messaging feature is really tricky to...\n",
       "1504                               Its not user friendly.\n",
       "1150    The loudspeaker option is great, the bumpers w...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[wrongs]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a decisiontree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76000000000000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=30).fit(X_train, y_train)\n",
    "dtc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69499999999999995"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(5).fit(X_train, y_train)\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome. Why do we think that MultinomialNB performs relatively well here?\n",
    "\n",
    "Why better than Decision Trees or KNeighbors?\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "1) Implement a MultinomialNB classifier using CountVectorizor with the options:\n",
    "max_features, min_df, and max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x505 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 4565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer(max_features=?)\n",
    "count_vect_1 = CountVectorizer(max_features=1000,\n",
    "                               min_df=3,\n",
    "                               max_df=50\n",
    "                              )\n",
    "bag_o_words_1 = count_vect_1.fit_transform(data['text'])\n",
    "bag_o_words_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80500000000000005"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1,X_test_1,y_train_1,y_test_1 = train_test_split(bag_o_words_1,data['sentiment'], random_state=42,test_size=0.2)\n",
    "clf_1 = MultinomialNB().fit(X_train_1, y_train_1)\n",
    "clf_1.score(X_test_1,y_test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Try with Tfidf, using the same settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81499999999999995"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_tfidf = TfidfVectorizer(max_features=1000,\n",
    "                                   min_df=3,\n",
    "                                   max_df=50\n",
    "                                  )\n",
    "bag_o_words_tfidf = count_vect_tfidf.fit_transform(data['text'])\n",
    "bag_o_words_tfidf\n",
    "\n",
    "X_train_tfidf,X_test_tfidf,y_train_tfidf,y_test_tfidf = train_test_split(bag_o_words_tfidf,data['sentiment'], random_state=42,test_size=0.2)\n",
    "clf_tfidf = MultinomialNB().fit(X_train_tfidf, y_train_tfidf)\n",
    "clf_tfidf.score(X_test_tfidf,y_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which was better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making this process faster\n",
    "\n",
    "Ok so if you're still with me (I hope you are!) then you will have realized that Exercise 1 was rather tedious. Data Scientists agree. Thus there is a framework in sklearn that combines GridSearch with Pipelines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resetting our data\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['text'],data['sentiment'], random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('clf', MultinomialNB()) ])\n",
    "fit_pipe = text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83999999999999997"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    107\n",
       "0.0     93\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.84      0.82      0.83        93\n",
      "   positive       0.84      0.86      0.85       107\n",
      "\n",
      "avg / total       0.84      0.84      0.84       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "predicted = fit_pipe.predict(X_test)\n",
    "print classification_report(y_test,predicted,target_names=['negative','positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we've repeated exactly what we did before. What's the point? (Hint: grid search is the point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {'vect__min_df':[1,2,3],\n",
    "              'vect__max_df':[5,10,100,1000],\n",
    "             'clf__alpha':[.01,1,100]}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Note: if you have a windows machine, keep n_jobs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit_grid = gs_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84499999999999997"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1, 'vect__max_df': 100, 'vect__min_df': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Rebuild the Pipeline using TfidfVectorizor and Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                     ('clf', DecisionTreeClassifier()) ])\n",
    "fit_pipe = text_clf.fit(X_train,y_train)\n",
    "fit_pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your pipe with GridSearch and see how it performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "parameters = {'vect__min_df':[1,2,3],\n",
    "              'vect__max_df':[5,10,100,1000],\n",
    "             'clf__max_depth':[1,30,100,1000]}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=1)\n",
    "fit_grid = gs_clf.fit(X_train,y_train)\n",
    "\n",
    "print fit_grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': 1000, 'vect__max_df': 100, 'vect__min_df': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus!\n",
    "\n",
    "There is extra data to test this on: imdb_labelled.txt and yelp_labelled.txt.\n",
    "\n",
    "Try out the pipelines there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pripeline with CountVectorizer Score: 0.7850 \n",
      "Pripeline with TfidfVectorizer Score: 0.7200 \n"
     ]
    }
   ],
   "source": [
    "# IMDB\n",
    "rows = []\n",
    "with open('../data/imdb_labelled.txt') as f:\n",
    "    for i,line in enumerate(f.readlines()):\n",
    "        row = line.split('\\n')[0].split('\\t')\n",
    "        if row[1] == '':\n",
    "            row[1] = np.nan\n",
    "        else:\n",
    "            row[1] = int(row[1])\n",
    "        rows.append(row)\n",
    "\n",
    "        \n",
    "all_imdb_data = pd.DataFrame(rows,columns=['text','sentiment'])\n",
    "#all_imdb_data.info()\n",
    "\n",
    "data_imdb = all_data.dropna()\n",
    "#data_imdb.info()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data_imdb['text'],data_imdb['sentiment'], random_state=42,test_size=0.2)\n",
    "\n",
    "text_clf_imdb = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('clf', DecisionTreeClassifier()) ])\n",
    "fit_pipe1 = text_clf_imdb.fit(X_train,y_train)\n",
    "print \"Pripeline with CountVectorizer Score: %.4f \" % fit_pipe1.score(X_test,y_test)\n",
    "\n",
    "text_clf_tfidf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                           ('clf', DecisionTreeClassifier()) ])\n",
    "fit_pipe2 = text_clf_tfidf.fit(X_train,y_train)\n",
    "print \"Pripeline with TfidfVectorizer Score: %.4f \" % fit_pipe2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pripeline with CountVectorizer Score: 0.8050 \n",
      "Pripeline with TfidfVectorizer Score: 0.7050 \n"
     ]
    }
   ],
   "source": [
    "# YELP\n",
    "rows = []\n",
    "with open('../data/yelp_labelled.txt') as f:\n",
    "    for i,line in enumerate(f.readlines()):\n",
    "        row = line.split('\\n')[0].split('\\t')\n",
    "        if row[1] == '':\n",
    "            row[1] = np.nan\n",
    "        else:\n",
    "            row[1] = int(row[1])\n",
    "        rows.append(row)\n",
    "\n",
    "        \n",
    "all_yelp_data = pd.DataFrame(rows,columns=['text','sentiment'])\n",
    "#all_yelp_data.info()\n",
    "\n",
    "data_yelp = all_data.dropna()\n",
    "#data_yelp.info()\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(data_yelp['text'],data_yelp['sentiment'], random_state=42,test_size=0.2)\n",
    "\n",
    "\n",
    "text_clf_yelp = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('clf', DecisionTreeClassifier()) ])\n",
    "fit_pipe3 = text_clf_yelp.fit(X_train,y_train)\n",
    "print \"Pripeline with CountVectorizer Score: %.4f \" % fit_pipe3.score(X_test,y_test)\n",
    "\n",
    "text_clf_tfidf = Pipeline([('vect', TfidfVectorizer()),\n",
    "                           ('clf', DecisionTreeClassifier()) ])\n",
    "fit_pipe4 = text_clf_tfidf.fit(X_train,y_train)\n",
    "print \"Pripeline with TfidfVectorizer Score: %.4f \" % fit_pipe4.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
